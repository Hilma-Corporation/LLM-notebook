{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f221ed47-e763-4b28-963d-55b8dc0a7fdd",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Hilma-Corporation/logo/main/logo_transparent_background.png\" alt=\"Description de l'image\" style=\"width: 100%; max-width: 600px; height: auto;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602f8c40-3697-43e4-b906-660de5c73a05",
   "metadata": {},
   "source": [
    "**Notice:**\n",
    "\n",
    "This code is the property of Hilma Corporation. Reproduction, distribution, or use of this code without the explicit consent of Hilma Corporation is strictly prohibited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e41c966-439f-424f-af4b-7174f20440e9",
   "metadata": {},
   "source": [
    "### Description of the Model HILMA-FIN-7B-8L\n",
    "\n",
    "**Model Name**: HILMA-FIN-7B-8L\n",
    "\n",
    "**Base Architecture**: Mistral-7B-Instruct-v0.2\n",
    "\n",
    "**Model Type**: Language Model\n",
    "\n",
    "**Model Size**:\n",
    "- **Total Parameters**: 7242.584 million (7.2 billion)\n",
    "- **Trainable Parameters**: 0.852 million (852,000)\n",
    "\n",
    "**Techniques Used**:\n",
    "- **LoRA (Low-Rank Adaptation)**: A method for efficiently adapting large language models by adding a limited number of new low-rank layers to the existing layers.\n",
    "\n",
    "### Training Configuration\n",
    "\n",
    "**Command Arguments**:\n",
    "- `--train`: Indicates that the model should be trained.\n",
    "- `--model mistralai/Mistral-7B-Instruct-v0.2`: Uses the pre-trained model \"Mistral-7B-Instruct-v0.2\" as the starting point.\n",
    "- `--data /Users/simon-pierreboucher/Desktop/data_llm/mlx-examples/lora/data`: Specifies the path to the training data.\n",
    "- `--batch-size 2`: Uses a batch size of 2 for training.\n",
    "- `--lora-layers 8`: Indicates that 8 LoRA layers will be added to the model.\n",
    "- `--iters 1000`: Performs 1000 iterations of training.\n",
    "\n",
    "### Purpose of the Model\n",
    "\n",
    "**HILMA-FIN-7B-8L** is designed to be a specialized model in the fields of finance, insurance, and real estate. By using a solid base like the `Mistral-7B-Instruct-v0.2` model and adapting it with LoRA techniques, you can finely tune the model's capabilities for specific tasks and particular data in your domain.\n",
    "\n",
    "### Training Process\n",
    "\n",
    "1. **Loading the Base Model**: The `Mistral-7B-Instruct-v0.2` model is loaded with its initial parameters.\n",
    "2. **Adding LoRA Layers**: 8 new LoRA layers are integrated into the model to allow fine-tuning.\n",
    "3. **Preparing Data**: Training data is loaded from the specified path.\n",
    "4. **Training**: The model is trained with a batch size of 2 over 1000 iterations.\n",
    "5. **Saving**: The fine-tuned model is saved for future use.\n",
    "\n",
    "### Benefits of HILMA-FIN-7B-8L\n",
    "\n",
    "- **Adaptability**: Thanks to the use of LoRA layers, the model can be finely tuned for specific tasks without requiring a complete retraining of the base model.\n",
    "- **Specialization**: By using training data specific to the fields of finance, insurance, and real estate, the model becomes an expert in these domains.\n",
    "- **Efficiency**: Adding LoRA layers allows the model to remain lightweight and computationally efficient while improving its performance on specific tasks.\n",
    "\n",
    "In summary, HILMA-FIN-7B-8L will be a powerful and specialized model, perfectly suited to the needs of finance, insurance, and real estate, thanks to an innovative and efficient approach to adapting language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fc55f1e-bcd9-48a6-8c31-d3dba03f8a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlx>=0.8.0 (from -r requirements.txt (line 1))\n",
      "  Using cached mlx-0.15.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (5.0 kB)\n",
      "Collecting transformers (from -r requirements.txt (line 2))\n",
      "  Using cached transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting numpy (from -r requirements.txt (line 3))\n",
      "  Using cached numpy-2.0.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting filelock (from transformers->-r requirements.txt (line 2))\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers->-r requirements.txt (line 2))\n",
      "  Using cached huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting numpy (from -r requirements.txt (line 3))\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting packaging>=20.0 (from transformers->-r requirements.txt (line 2))\n",
      "  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers->-r requirements.txt (line 2))\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 2))\n",
      "  Using cached regex-2024.5.15-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers->-r requirements.txt (line 2))\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers->-r requirements.txt (line 2))\n",
      "  Using cached safetensors-0.4.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers->-r requirements.txt (line 2))\n",
      "  Using cached tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers->-r requirements.txt (line 2))\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers->-r requirements.txt (line 2))\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.23.2->transformers->-r requirements.txt (line 2))\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers->-r requirements.txt (line 2))\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers->-r requirements.txt (line 2))\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers->-r requirements.txt (line 2))\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers->-r requirements.txt (line 2))\n",
      "  Using cached certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached mlx-0.15.2-cp312-cp312-macosx_14_0_arm64.whl (17.5 MB)\n",
      "Using cached transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Using cached huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "Using cached packaging-24.1-py3-none-any.whl (53 kB)\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl (165 kB)\n",
      "Using cached regex-2024.5.15-cp312-cp312-macosx_11_0_arm64.whl (278 kB)\n",
      "Using cached safetensors-0.4.3-cp312-cp312-macosx_11_0_arm64.whl (411 kB)\n",
      "Using cached tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl (2.4 MB)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tqdm, safetensors, regex, pyyaml, packaging, numpy, mlx, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed certifi-2024.6.2 charset-normalizer-3.3.2 filelock-3.15.4 fsspec-2024.6.1 huggingface-hub-0.23.4 idna-3.7 mlx-0.15.2 numpy-1.26.4 packaging-24.1 pyyaml-6.0.1 regex-2024.5.15 requests-2.32.3 safetensors-0.4.3 tokenizers-0.19.1 tqdm-4.66.4 transformers-4.42.3 typing-extensions-4.12.2 urllib3-2.2.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25a4a1b4-4f51-4e27-9397-a6e4fa7cc483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Loading pretrained model\n",
      "Fetching 11 files: 100%|█████████████████████| 11/11 [00:00<00:00, 15107.19it/s]\n",
      "Total parameters 7242.584M\n",
      "Trainable parameters 0.852M\n",
      "Loading datasets\n",
      "Training\n",
      "Iter 1: Val loss 2.837, Val took 12.957s\n",
      "Iter 10: Train loss 2.449, It/sec 1.711, Tokens/sec 652.850\n",
      "Iter 20: Train loss 1.658, It/sec 0.900, Tokens/sec 362.330\n",
      "Iter 30: Train loss 1.329, It/sec 1.267, Tokens/sec 542.517\n",
      "Iter 40: Train loss 1.197, It/sec 1.430, Tokens/sec 567.180\n",
      "Iter 50: Train loss 1.212, It/sec 1.481, Tokens/sec 607.569\n",
      "Iter 60: Train loss 1.155, It/sec 1.349, Tokens/sec 563.561\n",
      "Iter 70: Train loss 1.191, It/sec 1.455, Tokens/sec 567.836\n",
      "Iter 80: Train loss 1.107, It/sec 1.471, Tokens/sec 591.223\n",
      "Iter 90: Train loss 1.097, It/sec 1.531, Tokens/sec 549.014\n",
      "Iter 100: Train loss 1.082, It/sec 1.456, Tokens/sec 581.037\n",
      "Iter 100: Saved adapter weights to adapters.npz.\n",
      "Iter 110: Train loss 1.108, It/sec 1.460, Tokens/sec 585.095\n",
      "Iter 120: Train loss 1.043, It/sec 1.417, Tokens/sec 593.316\n",
      "Iter 130: Train loss 1.108, It/sec 1.452, Tokens/sec 563.032\n",
      "Iter 140: Train loss 1.108, It/sec 1.558, Tokens/sec 582.039\n",
      "Iter 150: Train loss 1.064, It/sec 1.473, Tokens/sec 582.030\n",
      "Iter 160: Train loss 1.087, It/sec 1.458, Tokens/sec 605.588\n",
      "Iter 170: Train loss 1.053, It/sec 1.480, Tokens/sec 566.151\n",
      "Iter 180: Train loss 1.107, It/sec 1.555, Tokens/sec 593.285\n",
      "Iter 190: Train loss 1.008, It/sec 1.451, Tokens/sec 553.776\n",
      "Iter 200: Train loss 1.118, It/sec 1.407, Tokens/sec 530.606\n",
      "Iter 200: Val loss 1.084, Val took 12.507s\n",
      "Iter 200: Saved adapter weights to adapters.npz.\n",
      "Iter 210: Train loss 1.112, It/sec 1.501, Tokens/sec 564.495\n",
      "Iter 220: Train loss 1.067, It/sec 1.537, Tokens/sec 603.127\n",
      "Iter 230: Train loss 1.013, It/sec 1.514, Tokens/sec 595.439\n",
      "Iter 240: Train loss 1.069, It/sec 1.554, Tokens/sec 593.371\n",
      "Iter 250: Train loss 1.067, It/sec 1.516, Tokens/sec 576.829\n",
      "Iter 260: Train loss 0.995, It/sec 1.499, Tokens/sec 564.062\n",
      "Iter 270: Train loss 1.078, It/sec 1.435, Tokens/sec 565.930\n",
      "Iter 280: Train loss 1.053, It/sec 1.483, Tokens/sec 544.387\n",
      "Iter 290: Train loss 0.960, It/sec 1.386, Tokens/sec 587.153\n",
      "Iter 300: Train loss 1.101, It/sec 1.491, Tokens/sec 550.405\n",
      "Iter 300: Saved adapter weights to adapters.npz.\n",
      "Iter 310: Train loss 1.039, It/sec 1.426, Tokens/sec 565.919\n",
      "Iter 320: Train loss 1.048, It/sec 1.584, Tokens/sec 573.861\n",
      "Iter 330: Train loss 1.047, It/sec 1.414, Tokens/sec 573.361\n",
      "Iter 340: Train loss 1.031, It/sec 1.372, Tokens/sec 570.597\n",
      "Iter 350: Train loss 0.982, It/sec 1.464, Tokens/sec 569.051\n",
      "Iter 360: Train loss 1.091, It/sec 1.451, Tokens/sec 546.869\n",
      "Iter 370: Train loss 0.988, It/sec 1.434, Tokens/sec 591.628\n",
      "Iter 380: Train loss 0.988, It/sec 1.450, Tokens/sec 585.980\n",
      "Iter 390: Train loss 1.048, It/sec 1.450, Tokens/sec 546.429\n",
      "Iter 400: Train loss 0.994, It/sec 1.390, Tokens/sec 559.148\n",
      "Iter 400: Val loss 1.058, Val took 12.511s\n",
      "Iter 400: Saved adapter weights to adapters.npz.\n",
      "Iter 410: Train loss 1.041, It/sec 1.463, Tokens/sec 570.108\n",
      "Iter 420: Train loss 0.958, It/sec 1.557, Tokens/sec 574.370\n",
      "Iter 430: Train loss 1.029, It/sec 1.500, Tokens/sec 574.922\n",
      "Iter 440: Train loss 1.057, It/sec 1.479, Tokens/sec 553.373\n",
      "Iter 450: Train loss 1.045, It/sec 1.503, Tokens/sec 557.308\n",
      "Iter 460: Train loss 1.082, It/sec 1.437, Tokens/sec 568.671\n",
      "Iter 470: Train loss 1.021, It/sec 1.512, Tokens/sec 601.334\n",
      "Iter 480: Train loss 1.030, It/sec 1.491, Tokens/sec 580.580\n",
      "Iter 490: Train loss 0.993, It/sec 1.420, Tokens/sec 598.075\n",
      "Iter 500: Train loss 1.044, It/sec 1.472, Tokens/sec 570.656\n",
      "Iter 500: Saved adapter weights to adapters.npz.\n",
      "Iter 510: Train loss 1.055, It/sec 1.452, Tokens/sec 573.308\n",
      "Iter 520: Train loss 0.994, It/sec 1.464, Tokens/sec 528.273\n",
      "Iter 530: Train loss 1.004, It/sec 1.448, Tokens/sec 559.942\n",
      "Iter 540: Train loss 0.940, It/sec 1.402, Tokens/sec 575.445\n",
      "Iter 550: Train loss 0.944, It/sec 1.400, Tokens/sec 566.375\n",
      "Iter 560: Train loss 0.929, It/sec 1.309, Tokens/sec 559.803\n",
      "Iter 570: Train loss 0.999, It/sec 1.477, Tokens/sec 550.826\n",
      "Iter 580: Train loss 0.967, It/sec 1.430, Tokens/sec 549.522\n",
      "Iter 590: Train loss 0.990, It/sec 1.534, Tokens/sec 564.873\n",
      "Iter 600: Train loss 1.013, It/sec 1.519, Tokens/sec 576.580\n",
      "Iter 600: Val loss 1.050, Val took 12.539s\n",
      "Iter 600: Saved adapter weights to adapters.npz.\n",
      "Iter 610: Train loss 0.996, It/sec 1.459, Tokens/sec 536.840\n",
      "Iter 620: Train loss 0.987, It/sec 1.563, Tokens/sec 551.173\n",
      "Iter 630: Train loss 0.953, It/sec 1.343, Tokens/sec 567.551\n",
      "Iter 640: Train loss 0.980, It/sec 1.449, Tokens/sec 575.381\n",
      "Iter 650: Train loss 0.939, It/sec 1.491, Tokens/sec 565.722\n",
      "Iter 660: Train loss 1.029, It/sec 1.736, Tokens/sec 536.897\n",
      "Iter 670: Train loss 0.906, It/sec 1.485, Tokens/sec 579.621\n",
      "Iter 680: Train loss 0.946, It/sec 1.377, Tokens/sec 572.900\n",
      "Iter 690: Train loss 0.909, It/sec 1.416, Tokens/sec 567.136\n",
      "Iter 700: Train loss 0.934, It/sec 1.474, Tokens/sec 596.487\n",
      "Iter 700: Saved adapter weights to adapters.npz.\n",
      "Iter 710: Train loss 0.969, It/sec 1.387, Tokens/sec 575.277\n",
      "Iter 720: Train loss 0.997, It/sec 1.443, Tokens/sec 559.151\n",
      "Iter 730: Train loss 0.962, It/sec 1.454, Tokens/sec 562.702\n",
      "Iter 740: Train loss 0.991, It/sec 1.386, Tokens/sec 564.364\n",
      "Iter 750: Train loss 0.955, It/sec 1.501, Tokens/sec 604.250\n",
      "Iter 760: Train loss 0.974, It/sec 1.507, Tokens/sec 597.813\n",
      "Iter 770: Train loss 1.020, It/sec 1.505, Tokens/sec 563.705\n",
      "Iter 780: Train loss 0.996, It/sec 1.394, Tokens/sec 578.203\n",
      "Iter 790: Train loss 0.961, It/sec 1.319, Tokens/sec 569.399\n",
      "Iter 800: Train loss 0.965, It/sec 1.517, Tokens/sec 574.726\n",
      "Iter 800: Val loss 1.039, Val took 12.530s\n",
      "Iter 800: Saved adapter weights to adapters.npz.\n",
      "Iter 810: Train loss 0.934, It/sec 1.445, Tokens/sec 543.091\n",
      "Iter 820: Train loss 0.968, It/sec 1.403, Tokens/sec 586.652\n",
      "Iter 830: Train loss 0.956, It/sec 1.473, Tokens/sec 597.001\n",
      "Iter 840: Train loss 0.965, It/sec 1.502, Tokens/sec 580.228\n",
      "Iter 850: Train loss 0.936, It/sec 1.504, Tokens/sec 554.524\n",
      "Iter 860: Train loss 0.953, It/sec 1.447, Tokens/sec 571.682\n",
      "Iter 870: Train loss 0.920, It/sec 1.422, Tokens/sec 581.083\n",
      "Iter 880: Train loss 0.968, It/sec 1.385, Tokens/sec 582.600\n",
      "Iter 890: Train loss 0.953, It/sec 1.393, Tokens/sec 561.933\n",
      "Iter 900: Train loss 0.905, It/sec 1.484, Tokens/sec 544.469\n",
      "Iter 900: Saved adapter weights to adapters.npz.\n",
      "Iter 910: Train loss 0.971, It/sec 1.485, Tokens/sec 569.469\n",
      "Iter 920: Train loss 0.912, It/sec 1.515, Tokens/sec 548.756\n",
      "Iter 930: Train loss 0.961, It/sec 1.457, Tokens/sec 596.613\n",
      "Iter 940: Train loss 0.961, It/sec 1.336, Tokens/sec 563.864\n",
      "Iter 950: Train loss 0.923, It/sec 1.439, Tokens/sec 575.188\n",
      "Iter 960: Train loss 0.974, It/sec 1.494, Tokens/sec 576.573\n",
      "Iter 970: Train loss 0.958, It/sec 1.488, Tokens/sec 569.849\n",
      "Iter 980: Train loss 0.990, It/sec 1.380, Tokens/sec 555.549\n",
      "Iter 990: Train loss 1.005, It/sec 1.575, Tokens/sec 546.225\n",
      "Iter 1000: Train loss 0.936, It/sec 1.482, Tokens/sec 581.488\n",
      "Iter 1000: Val loss 1.029, Val took 12.516s\n",
      "Iter 1000: Saved adapter weights to adapters.npz.\n"
     ]
    }
   ],
   "source": [
    "!python lora.py \\\n",
    " --train \\\n",
    " --model mistralai/Mistral-7B-Instruct-v0.2 \\\n",
    " --data /Users/simon-pierreboucher/Desktop/data_llm/mlx-examples/lora/data \\\n",
    " --batch-size 2 \\\n",
    " --lora-layers 8 \\\n",
    " --iters 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e05df-c253-4c69-a9fa-fd0e46bec18d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
